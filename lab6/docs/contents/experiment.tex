\chapter{Experiment setups}
\indent
    Experiment setups are listed in this chapter including 2 parts, 
    DQN and DDPG.

\section{DQN}
\indent
    DQN details are listed in this section including 4 parts, 
    network, action selection, updating behavior network, and updating target network.

\subsection{Network}
\indent
    Since \textit{LunarLander-v2} includes 4 actions, 
    i.e., 0 (No-op), 1 (Fire left engine), 2 (Fire main engine), 3 (Fire right engine), 
    a network with output dimension 4 is built to predict Q value (See Listing \ref{dqn-network}).

\begin{lstlisting}[language=Python, caption={Python code of \textcolor{blue}{\textbf{Net}} of DQN.}, label={dqn-network}]
class Net(nn.Module):
    def __init__(self, 
        state_dim=8, action_dim=4, hidden_dim=(400, 300)) -> None:
        super().__init__()
        
        self.layers = nn.Sequential(
            nn.Linear(in_features=state_dim,
                out_features=hidden_dim[0]),
            nn.ReLU(inplace=True),
            nn.Linear(in_features=hidden_dim[0],
                out_features=hidden_dim[1]),
            nn.ReLU(inplace=True),
            nn.Linear(in_features=hidden_dim[1],
                out_features=action_dim)
        )

    def forward(self, x: torch.FloatTensor) -> torch.FloatTensor:
        return self.layers(x)\end{lstlisting}

\subsection{Action selection}
\indent
    During the play, action with maximum Q value with probability $\epsilon$ will be selected, 
    or random action is selected, otherwise. It's called $\epsilon$-greedy (See Listing \ref{dqn-select-action}).

\begin{lstlisting}[language=Python, caption={Python code of \textcolor{blue}{\textbf{select\_action}} of DQN.}, label={dqn-select-action}]
def select_action(self, state, epsilon, action_space):
    if random.random() > epsilon:
        with torch.no_grad():
            # Max element is in 2nd column (dim=1).
            state = torch.from_numpy(state).view(1, -1).to(self.device)
            return self._behavior_net(state).max(dim=1)[1].item()
    else:
        return action_space.sample()\end{lstlisting}

\subsection{Updating behavior network}
\indent
    To update behavior network, sample transitions (state, action, reward, next action, done) from replay memory, then
    do TD-learning, and finally calculate MSE loss from Q value and Q target (See Listing \ref{dqn-update-behavior-network-code} and Equation \ref{dqn-update-behavior-network-eqn}).

\begin{lstlisting}[language=Python, caption={Python code of \textcolor{blue}{\textbf{\_update\_behavior\_network}} of DQN.}, label={dqn-update-behavior-network-code}]
def _update_behavior_network(self, gamma):
    state, action, reward, next_state, done = self._memory.sample(
        self.batch_size, self.device)

    q_value = self._behavior_net(state).gather(dim=1, index=action.long())
    with torch.no_grad():
        q_next = self._target_net(next_state).max(dim=1)[0].view(-1, 1)
        q_target = reward + gamma * q_next * (1 - done)
        
    criterion = nn.MSELoss()
    loss = criterion(q_value, q_target)

    self._optim.zero_grad()
    loss.backward()
    nn.utils.clip_grad_norm_(self._behavior_net.parameters(), 5)
    self._optim.step()\end{lstlisting}
    
\begin{equation}\label{dqn-update-behavior-network-eqn}
    y_j = 
    \begin{cases}
        r_j & \text{if episode terminates at step } j + 1 \\
        r_j + \gamma\max_{a'}\hat{Q}\left(\phi_{j + 1}, a'; \theta\right) & \text{otherwise}
    \end{cases}
\end{equation}
        
\subsection{Updating target network}
\indent
    To update target network, copy weights of behavior network (See Listing \ref{dqn-update-target-network}).

\begin{lstlisting}[language=Python, caption={Python code of \textcolor{blue}{\textbf{\_update\_target\_network}} of DQN.}, label={dqn-update-target-network}]
def _update_target_network(self):
    self._target_net.load_state_dict(self._behavior_net.state_dict())\end{lstlisting}

\section{DDPG}
\indent
    DDPG details are listed in this section including 5 parts, 
    actor network, critic network, action selection, updating behavior network, and updating target network.

\subsection{Actor network}
\indent
    Since \textit{LunarLanderContinuous-v2} includes 2 engines, 
    i.e., main engine with range $[-1, 1]$ and left-right engine with range $[-1, 1]$, 
    a network with output dimension 2 is built (See Listing \ref{ddpg-actor-network}).

\begin{lstlisting}[language=Python, caption={Python code of \textcolor{blue}{\textbf{ActorNet}} of DDPG.}, label={ddpg-actor-network}]
class ActorNet(nn.Module):
    def __init__(self, 
            state_dim=8, action_dim=2, hidden_dim=(400, 300)) -> None:
        super().__init__()
        
        self.layers = nn.Sequential(
            nn.Linear(in_features=state_dim,
                out_features=hidden_dim[0]),
            nn.ReLU(inplace=True),
            nn.Linear(in_features=hidden_dim[0],
                out_features=hidden_dim[1]),
            nn.ReLU(inplace=True),
            nn.Linear(in_features=hidden_dim[1],
                out_features=action_dim), 
            nn.Tanh()
        )
        
    def forward(self, x: torch.FloatTensor) -> torch.FloatTensor:
        return self.layers(x)\end{lstlisting}

\subsection{Critic network}
\indent
    Critic network is used to predict Q value. Since the output is scalar, output dimension 1. (See Listing \ref{ddpg-critic-network}).

\begin{lstlisting}[language=Python, caption={Python code of \textcolor{blue}{\textbf{CriticNet}} of DDPG.}, label={ddpg-critic-network}]
class CriticNet(nn.Module):
    def __init__(self, 
            state_dim=8, action_dim=2, hidden_dim=(400, 300)) -> None:
        super().__init__()
        
        h1, h2 = hidden_dim
        self.critic_head = nn.Sequential(
            nn.Linear(state_dim + action_dim, h1),
            nn.ReLU(),
        )
        self.critic = nn.Sequential(
            nn.Linear(h1, h2),
            nn.ReLU(),
            nn.Linear(h2, 1),
        )

    def forward(self, 
            x: torch.FloatTensor, action: torch.FloatTensor) -> torch.FloatTensor:
        x = self.critic_head(torch.cat([x, action], dim=1))
        return self.critic(x)\end{lstlisting}

\subsection{Action selection}
\indent
    During the play, select action with actor network and add extra noise (See Listing \ref{ddpg-select-action}).

\begin{lstlisting}[language=Python, caption={Python code of \textcolor{blue}{\textbf{select\_action}} of DDPG.}, label={ddpg-select-action}]
def select_action(self, state, noise=True):
    with torch.no_grad():
        if noise:
            re = self._actor_net(torch.from_numpy(state).view(1, -1).to(self.device)) + \
                torch.from_numpy(self._action_noise.sample()).view(1, -1).to(self.device)
        else:
            re = self._actor_net(torch.from_numpy(state).view(1, -1).to(self.device))
            
    return re.to('cpu').numpy().squeeze()\end{lstlisting}

\subsection{Updating behavior network}
\indent
    To update behavior network, sample transitions (state, action, reward, next action, done) from replay memory, and calculate MSE loss from Q value of behavior network and Q target of target network to update critic network. Also, to maximize Q value by updating actor network $\mu$, calculate negative Q value expectation $\mathbb{E}[- Q(s, \mu(s))]$ (See Listing \ref{ddpg-update-behavior-network-code} and Equation \ref{ddpg-update-behavior-network-eqn}).

\begin{lstlisting}[language=Python, caption={Python code of \textcolor{blue}{\textbf{\_update\_behavior\_network}} of DDPG.}, label={ddpg-update-behavior-network-code}]
def _update_behavior_network(self, gamma):
    # Sample transitions batch.
    state, action, reward, next_state, done = self._memory.sample(
        self.batch_size, self.device)

    # Update critic.
    q_value = self._critic_net(state, action)
    with torch.no_grad():
       a_next = self._target_actor_net(next_state)
       q_next = self._target_critic_net(next_state, a_next)
       q_target = reward + gamma * q_next * (1 - done)
       
    criterion = nn.MSELoss()
    critic_loss = criterion(q_value, q_target)

    self._actor_net.zero_grad()
    self._critic_net.zero_grad()
    critic_loss.backward()
    self._critic_opt.step()

    # Update actor.
    action = self._actor_net(state)
    actor_loss = -self._critic_net(state, action).mean()

    self._actor_net.zero_grad()
    self._critic_net.zero_grad()
    actor_loss.backward()
    self._actor_opt.step()\end{lstlisting}

\begin{equation}\label{ddpg-update-behavior-network-eqn}
    y_i = r_i + \gamma Q'(s_{t + 1}, \mu'(s_{t + 1} | \theta^{\mu'}) | \theta^{Q'})
\end{equation}

\subsection{Updating target network}
\indent
    To update target network, \textbf{soft copy} weights of behavior network (See Listing \ref{ddpg-update-target-network}).

\begin{lstlisting}[language=Python, caption={Python code of \textcolor{blue}{\textbf{\_update\_target\_network}} of DDPG.}, label={ddpg-update-target-network}]
def _update_target_network(target_net, net, tau):
    for target, behavior in zip(target_net.parameters(), net.parameters()):
        target.data.copy_((1 - tau) * target.data + tau * behavior.data)\end{lstlisting}